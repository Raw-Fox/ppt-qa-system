{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raw-Fox/ppt-qa-system/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 1: Install Required Libraries (Run only once)\n",
        "!pip install python-pptx transformers ipywidgets fpdf accelerate --quiet"
      ],
      "metadata": {
        "id": "AmF3hiEzQlR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 2: Import libraries\n",
        "import os\n",
        "from pptx import Presentation\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from fpdf import FPDF\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "3_R1yTm2Qlkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 3: File Upload\n",
        "uploaded = files.upload()\n",
        "pptx_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "NjQ9lChPQnrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 4: Extract Text from PPT\n",
        "def extract_text_from_ppt(pptx_path):\n",
        "    prs = Presentation(pptx_path)\n",
        "    full_text = \"\"\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                full_text += shape.text.strip() + \"\\n\"\n",
        "    return full_text.strip()\n",
        "\n",
        "ppt_text = extract_text_from_ppt(pptx_path)"
      ],
      "metadata": {
        "id": "wdOgk5UkQwQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 5: Generate Summary using BART\n",
        "tokenizer_bart = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model_bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "6ZwZqT0eQ2sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text_bart(text, max_chunk_len=1024):\n",
        "    inputs = tokenizer_bart.batch_encode_plus([text], return_tensors='pt', max_length=max_chunk_len, truncation=True)\n",
        "    summary_ids = model_bart.generate(inputs['input_ids'], num_beams=4, max_length=200, early_stopping=True)\n",
        "    return tokenizer_bart.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "aqhKZHPqQ5P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppt_summary = summarize_text_bart(ppt_text)"
      ],
      "metadata": {
        "id": "zpfvieGgQ-Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 6: Load LLaMA Model\n",
        "hf_token = \"your_hf_token_here\"  # Replace this\n",
        "llama_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(llama_model_id, use_auth_token=hf_token)\n",
        "model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    use_auth_token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "4ScMvoV2RAQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 7: Prepare combined context for better accuracy\n",
        "context = f\"\"\"[Original Text]\\n{ppt_text}\\n\\n[Summary]\\n{ppt_summary}\"\"\""
      ],
      "metadata": {
        "id": "lfme_36yRNIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 8: Q&A Function\n",
        "qa_pairs = []"
      ],
      "metadata": {
        "id": "SUYpNo6YRQDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_llama(question):\n",
        "    prompt = f\"\"\"You are a helpful assistant. Answer the question based on the presentation content below:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\"\"\n",
        "    inputs = tokenizer_llama(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model_llama.generate(**inputs, max_new_tokens=300)\n",
        "    answer = tokenizer_llama.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer.split(\"Answer:\")[-1].strip()"
      ],
      "metadata": {
        "id": "StyPjMHGRTQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  STEP 9: Interactive Interface\n",
        "text_input = widgets.Textarea(placeholder='Type your question here')\n",
        "ask_button = widgets.Button(description='Ask Question', button_style='primary')\n",
        "generate_pdf_button = widgets.Button(description='Generate PDF & Stop', button_style='danger')\n",
        "output_box = widgets.Output()"
      ],
      "metadata": {
        "id": "tfr3Ga4CRWNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_ask_clicked(b):\n",
        "    with output_box:\n",
        "        question = text_input.value.strip()\n",
        "        if question:\n",
        "            answer = ask_llama(question)\n",
        "            qa_pairs.append((question, answer))\n",
        "            print(f\"\\nQ: {question}\\nA: {answer}\\n\")\n",
        "            text_input.value = \"\"\n",
        "        else:\n",
        "            print(\" Please enter a question.\")"
      ],
      "metadata": {
        "id": "lx5BpAF1RYW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_generate_pdf_clicked(b):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.multi_cell(0, 10, f\" Summary:\\n{ppt_summary}\\n\", align='L')\n",
        "\n",
        "    for i, (q, a) in enumerate(qa_pairs, 1):\n",
        "        pdf.multi_cell(0, 10, f\"\\nQ{i}: {q}\\nA{i}: {a}\", align='L')\n",
        "\n",
        "    pdf_path = \"QnA_Output.pdf\"\n",
        "    pdf.output(pdf_path)\n",
        "    with output_box:\n",
        "        print(\"\\n PDF generated and ready to download.\")\n",
        "    files.download(pdf_path)"
      ],
      "metadata": {
        "id": "c6e4q45KRcpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_button.on_click(on_ask_clicked)\n",
        "generate_pdf_button.on_click(on_generate_pdf_clicked)"
      ],
      "metadata": {
        "id": "9xlqWXb5RhBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(widgets.VBox([text_input, ask_button, generate_pdf_button, output_box]))"
      ],
      "metadata": {
        "id": "JA5qFnQLRitZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}